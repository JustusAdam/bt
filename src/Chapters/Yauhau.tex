%!TEX root = ../thesis.tex
\chapter{Yauhau}

\label{ch:Yauhau}

\yauhau{} is a compiler plugin for the Ohua compiler.
It was first introduced in the paper ``Å¸auhau: Concise Code and Efficient I/O Straight from Dataflow.''~\cite{ErtelGoensAdamEtAl2016}, where a more detailed account of the motivation, implementation and results may be found.
Following I will briefly outline the basic transformation in \yauhau{} which is necessary to understand the arising problems which I am solving in the Chapters following.

\yauhau{} hooks into the compiler after the Clojure source has been parsed and transformed into the dataflow IR.
It is a pure IR to IR transformation.
Additionally a map of context information is given to the plugin.
The precise nature of this context information will be explained in section \ref{ch:Context} and is irrelevant to for the basic \yauhau{} idea and transformation.

Fundamentally \yauhau{} performs a graph transformation on the dataflow graph it has been given in form of the IR.
The goal behind this transformation is to find sets of IO operations which are data independent from each other and execute them as a batch job.
This is functionally similar to the approach taken by Haxl\cite{Haxl:library:link}, a Haskell library, also with the aim of optimising IO batching and the inspiration for the original paper.
Haxl leverages the Monad and Applicative abstractions provided by the Haskell language and base library to propagate a list of parallel executable IO actions through the program path, blocking each path as soon as an IO action is encountered.
This is a dynamic way of finding parallel IO actions, aka a runtime data structure (Data.Sequence.Seq) is populated as the program executes.
\yauhau{} takes a different approach and statically finds parallel IO actions at compile time.

\section{Runtime execution}

In \yauhau{} as well as Haxl once very remaining program path depends on data from an IO action a new round of fetches is being executed.
The list of parallel fetches is grouped into sublists on the data source they are trying to access.
Each sublist is handed to the respective datasource which has to perform the fetch.
In \yauhau{} this process is inherently parallel and each datasource performs its request at the same time in parallel.
In Haxl the programmer can choose whether to perform an asynchronous or synchronous request.
A new experimental and optional feature in \yauhau{} is to do dynamic dispatching of the finished requests.
This means once a certain datasource returns with the data, computations only depending on that datasource, can continue immediately.

\section{Datasources}

Datasources can be arbitrary user defined targets for IO actions such as databases, network requests or filesystem IO.
However datasources must be defined separately and only for those defined datasources the optimisation can be leveraged.

\subsection{Haxl}

Defining a datasource in Haxl means defining a set of actions on the source in form of a GADT.

\begin{minted}{Haskell}
data FacebookRequest a where
    GetFriends :: UserID -> FacebookRequest [UserID]
    GetProfile :: UserID -> FacebookRequest Profile
\end{minted}

Every time you want to make a request in the program it has to be encoded into one of those actions when passed to the IO action executing function \texttt{dataFetch}.

Additionally, for the cache, the \texttt{Hashable} typeclass needs to be implemented on this type as well as the \texttt{DataSourceName} and \texttt{StateKey} typeclass to identify the source.

The actual execution of the IO action is defined in a typeclass called \texttt{DataSource} which also has to be defined on the request type.
The \texttt{DataSource} typeclass has a method called \texttt{fetch} which does the actual work of performing the desired IO action.

\begin{minted}{Haskell}
class DataSource u request where
    fetch :: ... -> [BlockedFetch request] -> PerformFetch
\end{minted}

As its type signature indicates the \texttt{fetch} method performs the IO action on multiple requests simultaneously and this is where the batching happens.

When the framework executes the actions each datasources gets all the requests for that particular datasource. Therefore all actions which can be batched together should be defined on the same datasource (in the same GADT) and actions which cant be batched should be defined on separate datasources.

\subsection{\yauhau{}}

\yauhau{} has a more loose way of working with datasources.
IO actions in \yauhau{} are objects of the \texttt{Request} class.

\begin{minted}{Java}
public final class Request<Payload, Return>{
    private final Paylod payload;
    private final IDataSource<Payload, Return> dataSource;
    public Request(Payload payload,
                   IDataSource<Payload, Return> dataSource)
                  { /* implementation omitted */ }
    /* omitted code */
}
\end{minted}

Each \texttt{Request} has two fields, an arbitrary Payload and a datasource.
Here the datasource, similarly to Haxl implements a Java interface \texttt{IDataSource} which provides a method for executing batched requests.

\begin{minted}{Java}
public interface IDataSource<Payload, Return> {
    Object getIdentifier();
    Iterable<Return> fetch(Iterable<Payload> requests);
}
\end{minted}

At runtime requests are sorted by the datasource they are paired with and then handed to the respective source as a single list of many requests.

\section{Compile time transformation}

The basic \yauhau{} transformation algorithm, as mentioned, operates entirely on a dataflow graph in form of the IR.
The transformation does not concern itself with the kinds of nodes in the graph except for one, the \texttt{fetch} node/function which is a special node provided by the \yauhau{} library.
Beyond that the algorithm only concerns itself with the structure of the graph.
This significantly simplifies the algorithm.
The \fetch{} node itself is a regular stateful function (See Figure~\ref{fig:fetch-implementation}) and works without the \yauhau{} transformation, although using it without the batching optimisation makes it much less useful.

\begin{figure}
\begin{minted}{Java}
public final class FetchOp<P, R> {
    @defsfn
    public R fetch(@ReadOnly Request<P, R> request) {
        ArrayList<P> l = new ArrayList<>();
        l.add(request.getPayload());
        Iterable<R> res = request.getDataSource().fetch(l);
        return res.iterator().next();
    }
}
\end{minted}
\caption{implementation of the \texttt{fetch} function}
\label{fig:fetch-implementation}
\end{figure}

\subsection{Round detection algorithm}

The goal is to find a minimal set of rounds.
A `round' is a set of \texttt{fetch} nodes with no data dependencies between the nodes.
Rounds must not overlap.
Hence the algorithm finds a minimal set of non overlapping sets of data independent \texttt{fetch nodes}.

After much consideration I decided to go for a very similar approach to Haxl by simulating running the graph and blocking code paths when a \texttt{fetch} node is encountered.
Simulating the execution of the program is done by tracking three sets of items.

\begin{description}
	\item[created] The set of created bindings tracks which pieces of data that flow between the nodes of the program have been created prior to a particular point in the program.
	\item[visited] The set of visited nodes tracks which nodes were part of a previous stage.
	\item[round] The current round consists of all \fetch{} nodes which were part of a previous stage.
	When a new fetch round is finalised it will consist of these \fetch{} nodes and the current round will be newly initialised empty.
\end{description}

Since bindings are unique in the dataflow IR we can track produced data by simply adding the respective binding to the set of created bindings.
The set of created bindings is initialised with the input parameters of our program.
From this point we traverse the graph in stages.
Each stage consists of the set of graph nodes where all inflowing data has previously been created and which was not in a previous stage.
The former condition is checked by checking, for each input binding, whether the respective binding is contained in the set of created bindings.
To ensure the latter condition a separate set of visited nodes is maintained and only nodes which are not members of this set are allowed in a new stage.
Once the stage has been computed all functions in the stage are added to the visited set.
Then all \fetch{} nodes are filtered from the stage and added to the current round.
All remaining nodes are being executed.
Simulating to executing the nodes is done by simply adding all produced binding (return bindings) of the node to the set of created bindings.
If there are no remaining nodes to execute in a stage we have reached a point where the remainder of the dataflow graph depends on IO actions, including the \fetch{} nodes.
Therefore the current round is maximal and we simulate executing all \texttt{fetch} nodes of the current round and dispatch a finalised fetch round.
Afterwards the current round set is initialised empty again.
This is done until we get an empty stage, at which point we dispatch the current round again, unless it is empty.

Adding the \fetch{} nodes to the set of visited nodes guarantees that rounds do not overlap.
Not simulating fetch execution before the round finishes guarantees that no round contains two fetches with data dependencies between them, since the dependent fetch would never have been in a stage before the round was finished since the required binding from the first fetch would not be created before the round finishes.
Finally since we only dispatch a round once no more executable nodes are found all other nodes have to be fully data dependent and this guarantees the set of rounds to me minimal minimal.
% TODO maybe actually add a proof by induction here
% Should be doable by looking at the last stage first and then working backwards to
% previous stages.

\subsection{Graph rewrite}

Once the rounds have been computed an accumulated fetch is inserted for every round.
For each round all fetch nodes in the round are removed from the graph.
An accumulator is inserted with the combined inputs of all fetch nodes and the combined outputs of those nodes in the same order. See Figure \ref{figure:yauhau-transformation}.

\subsection{Control flow considerations}

At runtime a node in the dataflow graph waits for inputs to arrive in channels.
Theres one channel per function argument.
When the runtime scheduler activates the operator one input packet is removed from each channel and the stateful function is called with these arguments.
This process repeats until the scheduler decides to deactivate the node or if one of the input channels is empty.

These semantics are the same as they would be in a typical program, where every argument to a function has to be present for its call.
In dataflow this is not as straight forward as in typical code.
For instance free variables in algorithms inside an smap have to be specially handled during execution.
Normally variable bindings are simply translated into a set of arcs from where the value in the binding was originally created to each site where the variable is used.
When the source emits a value it is sent down each arc once.
This poses a problem in the case of smap where the target of the arc is inside the smap and other inputs may receive many packets due to the mapping, whereas this input would only receive one.
What Ohua does to solve this is replicate sending the curried packet once for each element in the mapped collection.

In Yauhau we run into a similar problem.
We do not curry values but perform an operation which is the complete opposite, where we rewrite connections to lead to an accumulator.
What we do not consider during our graph transformation is how often any of the fetch operators we are merging are called in the program.
If we have fetches in different contexts and naively accumulate them we create a similar situation to the one described above, where the inputs to the accumulator receive very different amounts of arguments, an example can be seen in Figure~\ref{fig:naive-batching-context-problem}.
In the example provided we can see two issues.
When a collection, which is longer than 1 is given to smap the respective input to the accumulator would be provided with a piece of data for each item in the collection whereas the bottommost input would at the same time only receive a single piece of data.
At the same time the topmost two inputs come from inside an if which means that only \textbf{one} of those two accumulator inputs would receive data at any given time, whereas the other receives nothing.

\begin{figure}
    \includegraphics[width=\textwidth]{Figures/yauhau-transformation}
	\caption{base transformation}
	\label{figure:yauhau-transformation}
    \includegraphics[width=\textwidth]{../Figures/naive-transformation}
	\caption{Naive batching}
	\label{fig:naive-batching-context-problem}
\end{figure}

The structures which introduce issues like this are control flow structures, because they change how often or whether a particular section of code is executed.
Unfortunately the flow of control in a program is not structurally visible in a dataflow graph.
Therefore our accumulation transformation is unable to deal with this problem.

There needs to be an additional preparation step before the batching transformation to produce an intermediate graph with certain properties such that the batching transformation is possible.
The properties of the graph we want are that any fetch in the same round is control flow wise in the same context, aka gets called the same amount of times.
Ensuring this particular property is more easiest if we simple pull every fetch so far out of its control flow context that it appears in the root context of the program.
This would be analogous to pulling every fetch out into the main block of the \texttt{main} function in a C program, splitting the rest into parts between the fetches.
Once our nodes are located there every fetch is guaranteed to be only ever called once per invocation of the program as a whole.
Therefore our desired property of every fetch being executed the same amount of times holds and we can proceed with the transformation.

In the following Chapters I will explain how we handle \texttt{smap} (Chapter~\ref{ch:smap-transformation}) and \texttt{if} (Chapter~\ref{ch:if-transformation}) and how we generalise the two to define a pluggable, general transformation which ensures we handle nesting correctly (Chapter~\ref{ch:Context}).

% But first let me briefly explain the main idea behind those transformations.
