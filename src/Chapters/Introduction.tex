%!TEX root = ../thesis.tex
\chapter{Introduction}

\label{ch:Intro}


% TODO the intro is a bit twisted: normally an intro goes by this structure: problem, missing solutions, contributions, outline (I like it when the last two are co-mingled.)

In the paper ``Ÿauhau: Concise Code and Efficient I/O Straight from Dataflow.''~\cite{ErtelGoensAdamEtAl2016}, authored by Sebastian Ertel, Andrés Goens, Jeronimo Castrillon and myself, \yauhau{}, a plugin for the Ohua\cite{Ertel:2015:OID:2807426.2807431}\cite{Ohua:library:link} compiler, was introduced.
\yauhau{} rewrites a program to achieve efficient IO whilst allowing the programmer to write very concise and straight forward code.
It is inspired by a similar system called Haxl, see Chapter~\ref{ch:related-work}.
In the paper explained the basic idea of finding data independent reading IO actions and combining them into batched requests, caching requests as well as executing them concurrently.
Because of our compile time transformation model however \yauhau{} faces challenges when confronted with control flow structures such as \texttt{if} and iteration.
Much headway in solving these challenges was made in the paper itself, but a generalisation of handling control flow is needed now.

Also introduced in the paper is the notion of a mutative IO action in the form of \texttt{read} requests.
We already explained how we ensure results of mutative actions are visible to the read requests despite the existence of a cache, however there is still more semantic consistency to be desired with read requests.
Namely that when algorithm $a$ depends on the result of running algorithm $b$, all writes and reads in $b$ should be performed before reads or writes in $a$ are performed.
There is currently no guarantee that this would happen because of the dataflow semantics of the underlying system Ohua.

In this thesis I want to explain in much more detail the concrete ways in which I handle control flow in \yauhau{} and explain implementation considerations and decisions made during and after writing of the paper but which we unfortunately did not have space for in the paper itself.
Chapter~\ref{ch:smap-transformation} concerns itself with \texttt{smap} and Chapter~\ref{ch:if-transformation} explains in more detail the handling of \texttt{if}.
I also want to introduce the reader to a new generalised concept, called context (Chapter~\ref{ch:Context}), which has now been added to Ohua and allows the correct handling of nested control flow structures.

After that in Chapter~\ref{ch:trans-implementation} I will briefly describe some general implementation decisions for \yauhau{} and optimisations which were added.

Following the contexts I will introduce another recent addition to \yauhau{} which we hinted at briefly in the paper, side effects and how to handle them semantically (Chapter~\ref{ch:side-effects}).
This chapter introduces the concept of controlled side effects in a functional and parallel system such as Ohua by means of the \yauhau{} plugin and how we intend to preserve semantics with regards to caching as well as read-write conflicts, whilst enabling easy and straight forward modularity.

At the beginning however I will introduce the reader to the underlying Ohua framework (Chapter~\ref{ch:Ohua}) and the basics of the \yauhau{} plugin (Chapter~\ref{ch:Yauhau}), the latter of which includes a description of the basic IO batching transformation.

Contributions of this thesis are:
\begin{itemize}
    \item A more detailed description, including implementation considerations and decisions, of the \textbf{if and smap transformations}.
    \item The generalisation of non structurally emergent dataflow graph properties into a concept called \textbf{Context}, its detection and use to handle nested control flow in \yauhau{}.
    \item An optional graph transformation in \yauhau{} used to \textbf{preserve write semantics} in programs using the \yauhau{} batching.
    \item An evaluation of the effect of precomputed conditional branches on latency.
    \item New \textbf{experiments} showing the performance of the \yauhau{} plugin in comparison to the existing technologies Haxl and Muse in \textbf{programs with conditionals and mapping}.
    \item \textbf{Extensions to the random code generator}~\cite{Goens-rand-code-graph} used in the \yauhau{} paper to generate test programs.
    These extenstions enable \textbf{support for mapping operations} as well as different \textbf{generation methods for conditionals} in the code generator.
\end{itemize}

%
% This thesis intends to improve on the current implementation of this plugin, provide explanation as to why adjustments to the implemetation are necessary in the first place, document adjustments made to the Ohua core which enable the plugin to function and finally expand on the experimental evaluation started in the paper, providing comparative data for performance of our plugin against frameworks with similar functionality.
%
% \section{Fundamental Technologies}
%
% This section shall serve as a reference for technologies which this thesis and its implementation is based upon.
% After this section I presume the reader to be familiar with these technologies and concepts and shall not provide further explanation.
%
% \subsection{Clojure}
%
% Clojure is a functional, dynamically typed language targeting the JVM  which we use to express the higher level algorithms in Ohua.
% Bejond that I use Clojure to implement most of the compiler internal algorithms, including those described in this thesis.
% There will occasionally be code examples to illustrate certain algorithms I am describing, those will usually be written in the Clojure language.
%
% \subsection{Ohua}
%
% Ohua is an automated parallelization framework which combines low level efficiency as provided by the JVM with high level expressions as found in the clojure programming language.
% In Ohua algorithms are implemented in the expressive clojure language and in terms of so called stateful functions, performant and state heavy java code pieces.
% Algorithms implemented in this way will can be automatically disassembled into a data flow graph and parallelized by the Ohua runtime and scheduler.
